\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{hyperref}

\begin{document}

\begin{center}
{\Large CS224n Winter 2019 Homework 4}

\begin{tabular}{rl}
SUNet ID: & 05794739 \\
Name: & Luis Perez \\
Collaborators: &
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}
\begin{enumerate}[label=(\alph*)]
  \item In ``utils.py''.
  \item In ``model\_embeddings.py''.
  \item In ``nmt\_model.py''.
  \item In ``nmt\_model.py''.
  \item In ``nmt\_model.py''.
  \item In ``nmt\_model.py''.
  \item
    As per the code, the masks end-up setting the $e_{t,i} = -\infty$ at all positions corresponding to `pad' tokens. This corresponds to assigning $-\infty$ energy to the ``annotations'' (encoder hidden states) corresponding to our padded sequences. After running through a Softmax, this leads to a zero-probability for these states, which means they don't contribute to our overall attention vector $a_t$ since this is the result of a weighed (by the softmax probability) average of the encoder hidden states.

    It is necessary to use the masks in this way since the `pad' tokens are artificial additions (mainly for performance) which should not be used by our decoder to generate translations. This information is not only useless, but could harm the overall ability of the model to translate sentences since many tranlations could be mis-matched in length, leading to `pad' tokens being alinged with real worlds.
  \item
    Trained on Azure VM.
  \item TODO
  \item
    \begin{enumerate}[label=\roman*]
      \item Dot Product Attention
        \begin{itemize}
          \item \textbf{Advantage}: The biggest advantage is computational, as well as the intepretability of this mechanism. The computational complexity of dot product is $O(n)$. Furthermore, this mechanism is very intuitive, where values with high cosine-similarity to the query will receive the most attention.
          \item \textbf{Disadvantage}: One big disadvantage of this method is a practical one. In order to use this attention mechanism, the values and the query must be of the same dimension. Generally speaking, this is an unnecessary restriction. Additionally, there are not explicit parameters to learn for this attention step, therefore the model is more restricted.
        \end{itemize}
      \item Multiplicative Attention:
        \begin{itemize}
          \item \textbf{Advantage}: With the introduction of the weight matrix, there is no longer a restriction that the dimension of the query and the values must match. Furthermore, the weight matrix itself can be learned, allowing for more expressivity in the model (ie, a value vector that's close to the query vector can be given a low-score due to the linear transformation it first undergoes).
          \item \textbf{Disadvantage}: Computational more complex than dot product attention, and some level of interpretability is lost since now there is an additional linear transformation of the value vectors.
        \end{itemize}
      \item Additive Attention
        \begin{itemize}
          \item \textbf{Advantage}: Explicitly allows for tuning the ``attention dimensionality'' thereby allow for techniques which try to bottleneck or expand the capacitor of the attention mechanism.
          \item \textbf{Disadvantage}: The most complex of all of the mechanisms, with the least level of interpretability.
        \end{itemize}
    \end{enumerate}
\end{enumerate}

\section*{Problem 1}
\begin{enumerate}[label=(\alph*)]
  \item 
    \begin{enumerate}[label=\roman*]
      \item 1
      \item 2
      \item 3
      \item
         \begin{enumerate}[label=\arabic*]
          \item The error in the NMT translation is that it's translating 
        \end{enumerate}
      \item
        \begin{enumerate}[label=\arabic*]
          \item The error in the NMT translation is that it's translating 
        \end{enumerate}
      \item 
        \begin{enumerate}[label=\arabic*]
          \item 
        \end{enumerate}
    \end{enumerate}
  \item TODO
  \item 
    \begin{enumerate}[label=\roman*]
      \item
    \end{enumerate}
\end{enumerate}


































\end{document}
